[{"content":"Notes about learning OS dev: [1] A minimal kernel Below are list of references that I used to learn\n A minimal Multiboot Kernel HelloOS  What is the minimal setting/configuration we need to have so we can boot a minimal kernel? Assuming we are using GRUB boot loader, it seems to me we need to have 4 files to be able to boot a minimal kernel.\n… ├── Makefile └── src └── arch └── x86_64 ├── multiboot_header.asm ├── boot.asm ├── linker.ld └── grub.cfg  Let\u0026rsquo;s go through these files one by one.\nmultiboot_header.asm To know what is this, we need to know how boot works. (assuming we use BIOS firmware standard)\n ┌────────┐ ┌─────────┐ ┌────────────────────────────┐ ┌───────────┐ │ │ │ │ │ │ │ │ │Power On├──────►│Load BIOS├──────►│Switch control to Bootloader├───────►│Load kernel│ │ │ │ │ │ │ │ │ └────────┘ └─────────┘ └────────────────────────────┘ └───────────┘  In general, we will be using an existing bootloader, such as GRUB (which uses multiboot specification). In order to tell the bootloader our kernel will support the multiboot specification, we will need to have a multiboot header to let the bootloader that we are supporting multiboot.\nsection .multiboot_header header_start: dd 0xe85250d6 ; magic number (multiboot 2) dd 0 ; architecture 0 (protected mode i386) dd header_end - header_start ; header length ; checksum dd 0x100000000 - (0xe85250d6 + 0 + (header_end - header_start)) ; small hack to avoid compiler warning ; insert optional multiboot tags here ; required end tag dw 0 ; type dw 0 ; flags dd 8 ; size header_end: ; end of the file  boot.asm Now the bootloader understands that we are using multiboot, what\u0026rsquo;s the next?\nWe need to add some code that bootloader can call. The way we do it is using boot.asm\nglobal start ; export a lable section .text ; default section for executing code bits 32 ; specify the following lines are 32 bit execution start: ; print `OK` to screen mov dword [0xb8000], 0x2f4b2f4f hlt ; tell CPU to stop  This basically defines a \u0026ldquo;function\u0026rdquo; but in assembly, it \u0026ldquo;prints\u0026rdquo; \u0026ldquo;OK\u0026rdquo; to screen and halt the CPU.\nlinker.ld Although the previous two steps, we have some assembly code, but we just prepare some seperate bits for different purposes, but they are not working together yet. We will combine them into an ELF executable so it can be run by the GRUB bootloader. The way we do this is through a linker.ld file.\nENTRY(start) /*entry point of the kernel*/ SECTIONS { . = 1M; /*conventional load address*/ .boot : { /* ensure that the multiboot header is at the beginning */ *(.multiboot_header) } .text : { *(.text) } }  The commands we can run to get the ELF executable.\n\u0026gt; nasm -f elf64 multiboot_header.asm \u0026gt; nasm -f elf64 boot.asm \u0026gt; ld -n -o kernel.bin -T linker.ld multiboot_header.o boot.o  Note: It\u0026rsquo;s important to pass the -n (or \u0026ndash;nmagic) flag to the linker, which disables the automatic section alignment in the executable. Otherwise the linker may page align the .boot section in the executable file. If that happens, GRUB isn\u0026rsquo;t able to find the Multiboot header because it isn\u0026rsquo;t at the beginning anymore.\nAfter this step, we have an ELF executable called kernel.bin. But how can we run this kernel? This would require our to go to next step.\ngrub.cfg The grub.cfg is a config file of GRUB. And this file can also be used to create ISO. It is very simple to create an ISO.\nCreate a new ISO approach  create a folder structure like this  isofiles └── boot ├── grub │ └── grub.cfg └── kernel.bin  What\u0026rsquo;s in the grub.cfg?  set timeout=0 set default=0 menuentry \u0026quot;my os\u0026quot; { multiboot2 /boot/kernel.bin boot }  run command grub-mkrescue -o os.iso isofiles. We will get an os.iso to be used for boot.  For example, we can boot our OS using qemu-system-x86_64 -cdrom os.iso\n Using host OS to boot However, we can also load the ELF file on exist OS without creating an ISO file. To add the option to boot from our new OS, we can add the following text in /boot/grub/grub.cfg\nmenuentry \u0026quot;my os\u0026quot; { insmod part_msdos insmod ext2 set root='hd0,msdos4' # `df /boot/` -\u0026gt; `/dev/sda4` -\u0026gt; `hd0,msdos4` multiboot2 /boot/kernel.bin boot }  Then we copy our kernel.bin into /boot/. If we reboot. This should work as well.\nConclusion To summarize, we will mainly need 4 parts to be able to boot from a minimal kernel.\n we need to tell bootloader GRUB what specification we are using (eg. mutiboot specification) -\u0026gt; multiboot.asm we need to have a kernel program that can be run -\u0026gt; boot.asm we need to create a ELF executable (combine header + kernel program) -\u0026gt; kernel.bin we need to have a config to tell GRUB how to install/run the OS we created -\u0026gt; grub.cfg  ","date":"2021-05-17","permalink":"https://lmjw.github.io/post/2021-05-17-notes-about-learning-os-dev/","tags":["OS","kernel"],"title":"Learning OS dev notes"},{"content":"Data Oriented Design Check out the video. This explains the AOS and SOA very well. data orientied design\n The following code is a common pattern in Object oriented programming.\nstruct Entity{ v3 postion, v3 velocity, int flag, virtual void update() } struct Player: public Entity{ float life, float mana, void update() override; } struct Monster: public Entity{ float life, void update() override; } sturct Door: public Entity{ bool current_status, float open_target, void update(); }  There are few problems with this approach:\n  Memory allocation. In this objected orientied approach, because we inherited Entity objects and added some fields in child object, the size of the new Object can have different sizes. So, we may ends up as many different size objects which will imply random heap allocation in memory. (just like objects are putting every where in memory without organize) So when we want to access these objects, the speed of accessing all these object can be a bit slow.\n  L1 Cache miss. Let\u0026rsquo;s say we want to update the postion of a player, we use the equation 'p = p + v*dt. To update the object state, we will need to know two fields stored in the obejct, the position and the velocity of the object. But, to access these fields, we will need to get all the structure in the L1 cache. So in the end, we fetched all the struct into L1 cache, but we only used two fields. This is something called caches misses.\n  The object oriented approach is also messy in terms of state management, and this can be expensive. Say a player hit a monster, the monster has its own state like how much life it has. But because of object oriented design, the state is kept private to monster, so eventually monster will need to have a method like get_damage. Eventually, we will need to load player object and then call the get_damage method of monster object. But what all we do is probably manipulate the life field, but we will need to fully load the player struct and monster struct, which may contain many other fields and method.\n   So, what is data oriented design and how can it solve this problem?\nThe difference of object oriented design and data oriented design is a difference between Array Of Struts(AOS) and Strut of Arrays(SOA). Lets see how AOS and SOA are stored in memory\nstruct ood{ type_A a, type_B b, type_C c } struct dod{ type_A a[1000], type_B b[1000], type_C c[1000] } // memory // OOD (Object orentied design) // ...|a|b|c|...|a|b|c|...|a|b|c|... // // DOD (data orentied design) // ...|a|a|a|...|b|b|b|...|c|c|c|... // // say we need to update a\u0026amp;b field for all structs. The first OOD case, we will // need to go through all the structs, fetch the whole struct into L1 cache, then // update field a,b, then iterate all the structs. Although we read C field in the // L1 Cache, we did not use it at all. We have about 33% cach missing rate. This // number is more significant if the object is bigger. // // But for DOD case, we will just need to pull all field of a and b, and just updates // these fields. We have almost 100% cache usagage, which will be much faster.  For the latter case, because in the memory, the same data is more closely aligned with same type, therefore it is more easy for cpu to do hardware optimization(hardware prefetching). In addition to that, this kind of structure makes SIMD (single instruction mulitple data) very easy.\nWhat is SIMD?\nSIMD is kind like hardware circle for some instruction which runs really fast and in paralle. SIMD allows same instructs for different data to execute in paralle to speed up the computation. SIMD is supported by some modern CPUs. With data orentied design, it can better utilize the SIMD to do the calculation as all our data is aligned nicely in memory.\n","date":"2019-11-24","permalink":"https://lmjw.github.io/post/2019-11-24-data-oriented-design/","tags":["cpp"],"title":"Data Oriented Design"},{"content":" This is a note for MIT 6.S191 course\nlink\n course 1. Intro to deep learning  The Perceptron: Forward Propagation\nSingle layer neural network with tensorflow:\nfrom tf.keras.layers import * inputs = Inputs(m) hidden = Dense(d1)(inputs) outputs = Dense(d2)(hidden) model = Model(inputs, outputs)  This four lines of code computes the single layer NN.\n Deep Neural Network\nMore hidden layers\n Applying Neural Networks\n Quantifying Loss\nCompare Predicted loss vs actual loss\n Minimize loss\nDifferent loss functions\n Binary cross entropy loss  loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( model.y, model.pred ))  Mean squared error loss  loss = tf.reduce_mean( tf.square( tf.subtract(model.y, model.pred) ) )   Training Neural Network\nFind W matrices that results the minimum loss\n Gradient descent  # randomly initialize W weights = tf.random_normal(shape, stddev=sigma) # loop over the following two steps # 1. gradient grads = tf.gradients(ys=loss, xs=weights) # 2. update weights weights_new = weights.assign(weights - lr * grads) # until grads converge # return the weights   Neural Network in practice\n Learning rate is hard to set  try out Design adaptive learning rates  Methods  Momentum tf.train.MomentumOptimizer Adagrad tf.train.AdagradOptimizer Adadelta tf.train.AdadeltaOptimizer Adam tf.train.AdamOptimizer RMSProp tf.train.RMSPropOptimizer         Mini-batching Gradient descent\n can utilize GPU to do the gradient descent   The problem of Over-fitting\n Regularization\navoid over-fitting\nRegularization method\n Dropout randomly tf.keras.layers.Dropout(p=0.5) Early stopping. stop training before over-fitting  training loss should always decay if validation set loss start to grow, we will just stop training here     Core Fundation Review\n The Perception  y = g({x}*[W]+w0)   Neural Networks, how does it work\n  Training in practice, over-fitting, etc.\n  ","date":"2019-06-12","permalink":"https://lmjw.github.io/post/2019-06-12-intro-to-deep-learning-notes/","tags":["deep-learning","python"],"title":"Intro to deep learning notes"},{"content":"Open SSL certificate authority Statement: This is just a study notes in order to understand the Open SSL and some relating concepts. A lot of contents in this article are copied from Jamie Nguyen\u0026rsquo;s blog OpenSSL Certificate Authority\nArticle summary(without Certificate revocation lists) The following graph summarizes the relationship between different keys and certificates. Certificate authority  A certificate authority (CA) is an entity that signs digital certificates. Many websites need to let their customers know that the connection is secure, so they pay an internationally trusted CA (eg, VeriSign, DigiCert) to sign a certificate for their domain.\nIn some cases it may make more sense to act as your own CA, rather than paying a CA like DigiCert. Common cases include securing an intranet website, or for issuing certificates to clients to allow them to authenticate to a server (eg, Apache, OpenVPN).\n CA-ROOT-keypair  root key (ca.key.pem)[private] root certificate (ca.csrt.pem)[Public]   Typically, the root CA does not sign server or client certificates directly. The root CA is only ever used to create one or more intermediate CAs, which are trusted by the root CA to sign certificates on their behalf.\n root key can be used to create a root certificate.\n Use the root key (ca.key.pem) to create a root certificate (ca.cert.pem). Give the root certificate a long expiry date, such as twenty years. Once the root certificate expires, all certificates signed by the CA become invalid.\n Intermediate CA  An intermediate certificate authority (CA) is an entity that can sign certificates on behalf of the root CA. The root CA signs the intermediate certificate, forming a chain of trust.\nThe purpose of using an intermediate CA is primarily for security. The root key can be kept offline and used as infrequently as possible. If the intermediate key is compromised, the root CA can revoke the intermediate certificate and create a new intermediate cryptographic pair.\n   Create an intermediate key [private]\n  use the intermediate key to create a certificate signing request (CSR)\n  Create the intermediate certificate(cert, crt) (using the root key?)\n Input: .csr output: .crt(.cert)    Create the certification chain file\n   When an application (eg, a web browser) tries to verify a certificate signed by the intermediate CA, it must also verify the intermediate certificate against the root certificate. To complete the chain of trust, create a CA certificate chain to present to the application.\nTo create the CA certificate chain, concatenate the intermediate and root certificates together. We will use this file later to verify certificates signed by the intermediate CA.\n     Sign server and client using intermediate CA  We will be signing certificates using our intermediate CA. You can use these signed certificates in a variety of situations, such as to secure connections to a web server or to authenticate clients connecting to a service.\n   Create private key for server and client\n   Our root and intermediate pairs are 4096 bits. Server and client certificates normally expire after one year, so we can safely use 2048 bits instead.\n     Use the private key to create a certificate signing request (CSR)\n   The CSR details don’t need to match the intermediate CA. For server certificates, the Common Name must be a fully qualified domain name (eg, www.example.com), whereas for client certificates it can be any unique identifier (eg, an e-mail address). Note that the Common Name cannot be the same as either your root or intermediate certificate.\n     Sign the .csr file\n   To create a certificate, use the intermediate CA to sign the CSR. If the certificate is going to be used on a server, use the server_cert extension. If the certificate is going to be used for user authentication, use the usr_cert extension. Certificates are usually given a validity of one year, though a CA will typically give a few days extra for convenience.\n     Deploy the certificate\n   You can now either deploy your new certificate to a server, or distribute the certificate to a client. When deploying to a server application (eg, Apache),you need to make the following files available:\n ca-chain.cert.pem www.example.com.key.pem www.example.com.cert.pem  If you’re signing a CSR from a third-party, you don’t have access to their private key so you only need to give them back the chain file(ca-chain.cert.pem) and the certificate (www.example.com.cert.pem)\n     Certificate revocation list (CRL)  A certificate revocation list (CRL) provides a list of certificates that have been revoked. A client application, such as a web browser, can use a CRL to check a server’s authenticity. A server application, such as Apache or OpenVPN, can use a CRL to deny access to clients that are no longer trusted.\n I will stop here just because I do not need this bit for now. Will come back later if needed.\n","date":"2018-01-05","permalink":"https://lmjw.github.io/post/2018-01-05-openssl-basic-concepts/","tags":["openssl"],"title":"A explanation of Open ssl certification"},{"content":"A python example of realizing secure grpc communication Useful links and references: Here are some links that I found it can be helpful when I was trying to work out how to setup the python ssl communication.\nSecure gRPC with TLS/SSL, This is golang implementation\ncertstrap, a convienient tool to generate openssl keys and certificate\ngRPC authentication, the official guide of grpc\ngrpcio, python package, the source code and official document of grpc python\ngrpc golang ssl example, another golang example of ssl communication\nWhat is the difference between .pem , .csr , .key and .crt?, a stackexchange question which explains the concepts and differences of different types of files\nWhat is a Pem file and how does it differ from other OpenSSL Generated Key File Formats?, a stackexchange question which explains the concepts and differences of different types of files\nCannot connect to SSL server using IP address, this is a useful thread that helped me to solve the server certificate problem\nTLS Golang Server not working with Node-js and Python Client, this thread makes me to understand to set the server certificate to have the name that is the same as the hostname.\nGo to my GITHUB repository to see the source code implementation.\n","date":"2018-01-05","permalink":"https://lmjw.github.io/post/2018-01-05-python-ssl-grpc-implementation/","tags":["grpc","docker","python"],"title":"A python example of realizing secure grpc communication"},{"content":"First, build a docker container that contains all required packages. In this example, I choose ubuntu:bionic as the basic image, and I installed other packages onto it.\nFROM ubuntu:bionic RUN apt-get update RUN apt-get install python3 -y RUN apt-get install python3-pip -y RUN pip3 install grpcio ADD app /app/ EXPOSE 22222  The Dockerfile is shown above. In the app file, it contains 4 files. They are:\n client.py, server.py, test_pb2.py, test_pb2_grpc.py\n test_pb2.py and test_pb2_grpc.py is generated by compiling the test.protofile. The content for test.proto is shown below.\nsyntax = \u0026quot;proto3\u0026quot;; package lmjwtest; // service, encode a plain text service EncodeService { // request a service of encode rpc GetEncode(plaintext) returns (encodetext) {} } message plaintext { string pttransactionID = 1; string ptproperties = 2; string ptsenderID = 3; } message encodetext { string enctransactionID = 1; string encproperties = 2; string encsenderID = 3; }  By using the grpcio-tools to compile the test.protofile, we can get the test_pb2.py and test_pb2_grpc.py two files.\nThe compile command is:\npython3 -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. test.proto  Then we create the server and client using the grpc generated python file.\nThe server.py file:\nfrom concurrent import futures import base64 import time import test_pb2 import test_pb2_grpc import grpc def encoding(msg): return base64.a85encode(msg.encode()) class EService(test_pb2_grpc.EncodeServiceServicer): def GetEncode(self, request, context): return test_pb2.encodetext(enctransactionID = encoding(request.pttransactionID), encproperties = encoding(request.ptproperties), encsenderID = request.ptsenderID) def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=2)) test_pb2_grpc.add_EncodeServiceServicer_to_server(EService(),server) server.add_insecure_port('[::]:22222') server.start() try: while True: time.sleep(60*60*24) except KeyboardInterrupt: server.stop(0) if __name__ == '__main__': serve()  the client.py:\nimport grpc import test_pb2 import test_pb2_grpc def run(): channel = grpc.insecure_channel('server:22222') stub = test_pb2_grpc.EncodeServiceStub(channel) response = stub.GetEncode(test_pb2.plaintext(pttransactionID = 'abcde', ptproperties = 'This is a plain text transaction', ptsenderID = 'Will smith')) print(\u0026quot;Encdded service received:\\n EnctransactionID:%s\\n,Encproperties:%s\\n,EncsenderID:%s\\n\u0026quot;%(response.enctransactionID,response.encproperties,response.encsenderID)) if __name__ == \u0026quot;__main__\u0026quot;: run()  To test the grpc on the local host, we can open two terminal. The first terminal runs the server.py and the second runs the client.py. If the client.py can return the encoded message, it proves the grpc is working properly on the local host.\n The Next step is to deploy this simple application on docker containers and the client and server need to be on different containers. So the encoding is considered as a microservice. To do this, we need to run two containers.\nUsing command docker build . to generate the docker images from the dockerfile. Note that you need to create an app folder and copy the \u0026ldquo;client.py, server.py, test_pb2.py, test_pb2_grpc.py\u0026rdquo; into this folder.\nthe file tree should look like this\n-somename -Dockerfile -app/ -client.py -server.py -test_pb2.py -test_pb2_grpc.py  run docker build . in the directory \u0026ldquo;somename\u0026rdquo;. This should create a docker image. Copy the created image ID image-id.\nUsing docker run -it image-id, replace the image-id with your image ID that was created by docker build. You need to do this twice in two different command line so that you have two different container. Find the two container ID. In the following code, I use container1 and container2 to identify two different containers id.\nThe next step is to link this two containers via an network. We can use docker network command to achieve this.\nopen a third terminal and type:\ndocker network create testnet  docker network connect testnet container1 --alias client docker network connect testnet container2 --alias server  If you have gone through the code carefully, you may wondering where was the \u0026lsquo;\u0026lsquo;server:22222\u0026quot; come from in client.py code. Well, here is it. The fact is we name the container2 using the network alias as \u0026ldquo;server\u0026rdquo; so all the containers in this network can use \u0026ldquo;server\u0026rdquo; to find this container. You can also use the container ID to replace \u0026ldquo;server\u0026rdquo;.\nSo now, you can run the command python3 app/server.py on the container2 and run python3 app/client.py on the container1. You should be able to see the client side successfully get the encoded message from server.\nWELL DONE! Now, we have realized the comunication between two containers with grpc!\n That\u0026rsquo;s all about this post. Let me know if you have any questions.\n","date":"2017-12-26","permalink":"https://lmjw.github.io/post/2017-12-26-docker-grpc-network/","tags":["Docker","grpc","network"],"title":"A simple example of using docker container to realize the grpc client and server communication"},{"content":"#Some concepts of Kubernetes Clusters\n  Kubernetes cluster(KC) can be analogy to the conventional computational cluster, which includes many computers and to work as a single unit.\n  application runs on the KC needs to be containerized.\n  the program organizing the KC, namely Kubernete, manage the deployment and make containerized application run in KC in a more efficient way.\n  Kubernetes cluster structure  Master ( act as a manager): coordinate the cluster Nodes (act as worker, the job of work is to run the containerize application)  Master responsibilities : coordinate activities in cluster  scheduling application maintaining application states scaling application rolling new updates  Node(worker)  definition of Node: A node can be a VM or a physical computer that serves as a worker machine in a KC. So, you can think node is just a physical laptop/desktop. What node have?    name description Analogy     Kubelet this guy does two thing: 1. managing the node; 2. communicate with master    Docker/rkt tool to handle containerized application(containers)       Kubernetes application deployment  Kubernetes deployment process   tell the master to start application containers. then master schedules the containers to run on the cluster\u0026rsquo;s nodes. master exposes the Kubernetes' API. The node also communicate with this API end user can use Kubernete\u0026rsquo;s API to interact with cluster Kubernetes can be deployed on either physical or vertial machines  Kubernetes development,Minikube   one simple Kubernetes implementation(simple cluster only contains one node) Minikube CLI provide basic bootstrapping operations for cluster (start, stop, status and delete)  Kubernetes interactive tutorial Module 1: Creating a cluster  check the minikube. Type the commands in terminal  minikube version  start a cluster  minikube start  to interact with Kubernetes, we will use the command line interface, kubectl. To check whether Kubectl is installed, you can run kubectl version check.  kubectl version  to see the cluster details, we can type the command  kubectl cluster-info  to view the cluster\u0026rsquo;s nodes, we can type the command  kubectl get nodes  ###Module 2. Deploy an Application using Kubectl\nKubernetes application deployments  running your Kubernetes cluster deploy containerized application on top of it. This requires you to create a Kubernetes Deployment configuration. Deployment configuration instruct Kubernetes how to create and update instances of your application. Once the deployment is created by you. the Kubernetes master loads the mentioned application instances into individual nodes.  After Kubernetes applications are deployed  once the application instances are created, a** Kubernetes Deployment Controller** continuously monitors those instances. if a node goes down or get deleted, the Deployment Controller replaces it. this provides a self-healing mechanism to address machine failure or maintainance.  Now, let\u0026rsquo;s try to deploy your first app on Kubernetes Never the less, here\u0026rsquo;s a diagram of showing the relationships between different components of Kubernetes cluster.  you can create and manage a deployment by using kubernetes command line interface, kubectl. when you create a deployment, you need to specify the container image for your application and how many replicas that you want to run.  First deployment example, Module 2  ** Node.js** application packaged in a Docker container. source code goal: deploy your first app on Kubernetes using kubectl.   Kubectl basic commands     command description     kubectl version get the version of kubectl   kubectl get nodes view the nodes in cluster    run an app using Kubectl  kubectl run \u0026lt;command\u0026gt;  example:\nkubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --port=8080  What this previous command did?\n find the available nodes that can be used to run the application schedule the application to run on that node configure the cluster to reschedule the instance on a new Node when needed  to list your deployments  kubectl get deployments  View our apps    pods: running inside Kubernetes are running on a private, isolated network. (What the fuck is this pods?). By default, they are visible from other pods and services within the same kubernetes cluster, but not outside that network.\n  kubectl can create a proxy that will forward communications into cluster-wide, private network. (But how? Show me the code). kubectl interacting through an API endpoint(Wtf?) to communicate with application.\n  This is how, we use a second terminal to open the proxy:\n  kubectl proxy  So, this is the understanding, kubectl is a piece of program that is run in the bash terminal(host pc). After the proxy execution, we now have a connection between our host(terminal) and the Kubernetes cluster. The proxy enables direct access to the API through terminals.\n Once this proxy is set, it means we have set up the communications between host and Kubernetes cluster. But how do we actually interact with the pod? what it actually happens, is that the proxy will automatically set up proxy endpoints (similar like http://proxy/endpoints) for each pods and we can actually query the APIs for individual pods by using internet protocols with curl command, for example.  You can see all those APIs hosted through the proxy endpoint, now available at through http://localhost:8001. For example, we can query the version directly through the API using the curl command: curl http://localhost:8001/version   The API server will automatically create an endpoint for each pod, based on the pod name, that is also accessible through the proxy. we can get all the pods name and store it in an environment variable POD_NAME (on the host) export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\u0026quot;\\n\u0026quot;}}{{end}}')      Summary of this section terms and concepts    Terms Concepts     Kubernetes Cluster (KC) The actual cluster that contains many nodes(VMs or physical PCs)   Kubernetes Sometimes indicate the program that organize the KC, managing the application deployment and scheduling.   Containerize Application is packed in a way that is independent from its environment   Master node A special node in the KC. Its main tasks are scheduling and controlling the applications that are running in the normal KC nodes. So, in this sense, the master node controls the normal nodes. In analogy, this nodes is like a manager   Nodes The actual PCs or VMs that are used for running the applications. In analogy, these nodes are like workers.   Kubelet A piece of software (called \u0026ldquo;agent\u0026rdquo; in Kubernetes official document) that taking control of an actual node, and managing the communication with master. Every node has a Kubelet.   Docker/rkt This is a tool to handle containerized applications. Since every node needs to run containerize application, node needs a tool that can be used to manage these applications.   minikube A simple Kubernetes cluster implementation. It only contains 1 Node.   kubectl The command line interface that is used to interact with the Kubernetes program. So this \u0026ldquo;kubectl\u0026rdquo; is actually independent of Kubernetes cluster and nodes.   Deployment configuration A configuration file that tells the Kubernetes Cluster how to configure and deploy your applications. Then the master node will then load this file and deploy the applications to nodes. The deployment can also be done by using the \u0026ldquo;kubectl\u0026rdquo; command line interface.   Deployment controller Once the application is deployed on nodes, a Deployment controller will take in charge to monitor all these applications. If nodes goes down or deleted, the deployment controller will replace it with new one.   pods(?) The concept of this is not quite clear yet. To me, it is more like a running instance of an application. There may be more than one instance of applications. In analogy, the pods is like an instance of a class, there are maybe several instances of same class. (dog = Animal(); cat = Animal(), dog and cat are similar to pods)   Proxy(?) The KC is more like a network, where as each pod is act like an user in the KC network. In the KC, a proxy build up a communication channel that allows the pods to be able to communicate with each other.   endpoint(?) Every pods can be linked to the proxy with an address. In my understanding, this address can be referred as endpoint.    ","date":"2017-12-06","permalink":"https://lmjw.github.io/post/2017-12-06-kubernetes/","tags":["Kubernetes","docker"],"title":"Kubernete's tutorial notes"},{"content":"Recently, I was trying to install GMP library on my windows 10 machine.\nThe first problem that I was facing is\n \u0026lsquo;gcc\u0026rsquo; is not recognized as an internal or external command, operable program or batch file.\u0026quot;\n Then I did some searches and I found this page which showed how to install GMP on windows. However, it is a bit outdated, I still encounter some problems in following its steps. Luckily I figured out how to solve the problems and installed the GMP successfully.\nIn the guide, they recommended to use Dev-C++ to get gcc compiler. I had tried this method, however it didn\u0026rsquo;t work. It returned some error when I was trying to run ./configure.\nHere I will suggest a different approach, instead of using the Dec-C++, I recommend to install MinGW following the guild from MinGW website. You need to install both MinGW and MSYS.\nAfter installation, if you open window cmd terminal, you may still not find \u0026ldquo;gcc\u0026rdquo;. The trick is to go to the directory (\u0026ldquo;C:\\MinGW\\msys\\1.0\u0026quot;) (if you install MinGW in its default directory) and double click \u0026ldquo;msys.bat\u0026rdquo; then a terminal is popped out. In this terminal, you will be able to find gcc command.\nThe next step is to use \u0026ldquo;cd\u0026rdquo; command to go to the GMP directory. (My path is :\u0026ldquo;C:\\c++\\gmp\\gmp-6.1.2\u0026quot;). In this directory, run \u0026ldquo;./configure\u0026rdquo; and it should work correctly. I did followed the guide to modify the make files, but I suspect it can work without changing.\n  Update fix You must fix an error which currently exists in the Makefiles. You must do one of the following:   **Before *running ./configure (step 4), go to C:\\c++\\GMP\\GMP-4.2.1\\mpn*Makeasm.am and go to the last line of the file. If you find \u0026ndash;m4=\u0026quot;$(M4)\u0026quot; in the middle of it, change it to \u0026ndash;m4=$(M4). That is, remove the double quote marks, After running ./configure (step 4), go to C:\\c++\\Includes\\GMP\\GMP-4.2.1\\mpn*Makefile* and also C:\\c++\\Includes\\GMP\\GMP-4.2.1\\tests*Makefile* and make the same replacements described just above, although not quite at the last line. Change \u0026ndash;m4=\u0026quot;$(M4)\u0026rdquo; to \u0026ndash;m4=$(M4).   Instead of run \u0026ldquo;./configure\u0026rdquo;, I did run \u0026ldquo;./configure \u0026ndash;enable-cxx\u0026rdquo;. However, in order to configure successfully with \u0026ldquo;enable cxx\u0026rdquo;, you need to install not only gcc, but also g++ compiler as well. To install g++ compiler, you can simply open \u0026ldquo;MinGW installation manager\u0026rdquo; and find mingw gcc g++ compiler and install it.\nAfter configure run successfully, you can type\nmake  and\nmake install  then finally\nmake check  I didn\u0026rsquo;t face any new issues. All the tests have been passed. So the gmp is successfully installed.\nSummary In summary, the trick is to install mingw and msys properly. If you follow mingw page properly, you should not experience any issue. (At least I didn\u0026rsquo;t). SO this basically the guild for installing GMP on windows 10.\n","date":"2017-11-30","permalink":"https://lmjw.github.io/post/2017-11-30-install-gmp-on-windows10-machine/","tags":["GMP"],"title":"Install GMP on windows 10 machine"},{"content":"Python subprocess examples Description This is a collection of python subprocess examples for easy usage. Although I found the python documents are very comprehensive, I found there is not much examples of showing how to use. This document act as a note of how to use python subprocess package.\nEnvironment settings    OS Windows 10 home     python Anaconda, python 3.6.3    Main functions   subprocess.run() the code is executed in IDLE command line window.\n\u0026gt;\u0026gt;\u0026gt; import subprocess as sb \u0026gt;\u0026gt;\u0026gt; bash_exec = \u0026quot;C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe\u0026quot; \u0026gt;\u0026gt;\u0026gt; sb.run([bash_exe, '-c','pwd'], stdout=sb.PIPE)   CompletedProcess(args=[\u0026lsquo;C:\\Program Files\\Git\\bin\\bash.exe\u0026rsquo;, \u0026lsquo;-c\u0026rsquo;, \u0026lsquo;pwd\u0026rsquo;], returncode=0, stdout=b'/c/Users/xxx/Anaconda3/Scripts\\n')\n NOTE:\n need to point to bash exe bash needs a \u0026quot;-c\u0026quot; condition in order to run it, otherwise a returncode=126 will return. Cannot using -i tag for bash command execution. return result will not be shown if stdout is not set.    subprocess.Popen(*) However, I personally feel that the Popen is the most important tool of this subprocess package.\nBasic syntex\n\u0026gt;\u0026gt;\u0026gt; p = sb.Popen([bash_exe, '-c', 'pwd'], stdout=sb.PIPE, stderr=sb.PIPE) \u0026gt;\u0026gt;\u0026gt; p.communicate()   (b'/c/Users/xxx/Anaconda3/Scripts\\n', b'')\n this is working.\nThe other one I thought it would work but actually not\n\u0026gt;\u0026gt;\u0026gt; p = sb.Popen(['-c','pwd'],executable=bash_exe, stdout=sb.PIPE, stderr=sb.PIPE) \u0026gt;\u0026gt;\u0026gt; p.communicate()   (b'', b'/usr/bin/pwd: /usr/bin/pwd: cannot execute binary file\\n')\n The error turn out to be a bit wired. For now, I am not sure how is this happened.\nenv might be one very useful attributes when some info needs to be preloaded before executing the command.\n\u0026gt;\u0026gt;\u0026gt; p = sb.Popen([bash_exe, '-c', 'echo $NAME'], stdout=sb.PIPE, stderr=sb.PIPE) \u0026gt;\u0026gt;\u0026gt; p.communicate()   (b'\\n', b'')\n \u0026gt;\u0026gt;\u0026gt; p = sb.Popen([bash_exe, '-c', 'echo $NAME'], stdout=sb.PIPE, stderr=sb.PIPE, env = {'NAME':'superman'}) \u0026gt;\u0026gt;\u0026gt; p.communicate()   (b\u0026rsquo;superman\\n', b'')\n context manager\nwith sb.Popen([bash_exe, '-c', 'echo $NAME'], stdout=sb.PIPE, stderr=sb.PIPE, env = {'NAME':'superman'}) as proc: proc.communicate()   (b\u0026rsquo;superman\\n', b'')\n Not working:\n\u0026gt;\u0026gt;\u0026gt;with sb.Popen([bash_exe, '-c',], stdout=sb.PIPE, stderr=sb.PIPE, stdin = sb.PIPE, env = {'NAME':'superman'}) as proc: proc.communicate(input=input()) pwd   (b'', b'/usr/bin/bash: -c: option requires an argument\\n')\n Eh, looks I can write a basic bash CLI?\nimport subprocess as sb command = '' kernal = \u0026quot;C:\\\\Program Files\\\\Git\\\\bin\\\\bash.exe\u0026quot; while command!=\u0026quot;exit\u0026quot;: command = input('%: ') proc = sb.Popen([kernal,'-c',command], stdout=sb.PIPE, stderr=sb.PIPE, encoding = 'utf-8') out,err = proc.communicate() if err =='': print(out) else: print(err)  However, if you input \u0026ldquo;cd\u0026rdquo; command, it actually doesn\u0026rsquo;t change its directory. SO, this CLI sucks.\nOne easy solution is to use some libraries such as Pespect on Windows.\nNOTE:\n(TODO)\nthis seems a good solution for building an interactive shell.\n​\n  ","date":"2017-11-28","permalink":"https://lmjw.github.io/post/2017-11-28-python-subprocess/","tags":["python","subprocess","CLI"],"title":"python subprocess"},{"content":"Description: Using golang to execute python code, which can in turn execute docker command to control docker container.\n The overall idea is to execute some python script in golang environment using golang exec, and this python script contain some docker commands (bash). So the overall flow of execution is:\n [bash terminal] ===\u0026gt; [golang.exec] ===\u0026gt; [python.subprocess] ===\u0026gt; [bash] ===\u0026gt; [docker command]\n The issue occurs.\n the input device is not a TTY. If you are using mintty, try prefixing the command with \u0026lsquo;winpty\u0026rsquo;\n TEST COMMAND docker run -it ubuntu bash\r #####Environment setting:\n windows 10 home docker toolbox Git: git version 2.9.0.windows.1 Python: Python 3.6.3, Anaconda custom (64-bit) golang: go version go1.9.2 windows/amd64  #####Failed example:\nrunning under default git-bash environment in windows 10:\n$ docker run -it ubuntu bash\r  \u0026gt;\u0026raquo; the input device is not a TTY. If you are using mintty, try prefixing the command with \u0026lsquo;winpty\u0026rsquo;\n Similar issue has been mentioned in this docker tutorial and was explained with a proposed solution by Will Anderson in his blog. #####Ideal successful case\n\u0026gt;\u0026gt;\u0026gt; go-command()\r# inside go-command()\rgo.exec(\u0026quot;python script\u0026quot;)\r# inside python script\rpython.subprocess(\u0026quot;bash\u0026quot;,\u0026quot;docker run -it ubuntu bash\u0026quot;)\r The idea is by execute a command in a CLI created by golang, it will call a execution of a python script, and this python script can then subprocess execute the docker run -it ubuntu bash in the bash terminal.\n####Experiment\n  the first experiment is to get rid of \u0026ldquo;is not a tty\u0026rdquo; error and make it work in python script. Because this command(docker run -it ubuntu bash) is not working in the git-bash terminal, it will never (I think) work if you open idle or any python CLI for this git-bash. So the way you open the python CLI is important.\nThe easiest solution is to use the Docker Quickstart Terminal to open the python CLI. In the working directory, you can create a simple python subprocess function and name this file pyCmd.py.\nfrom subprocess import Popen, PIPE, STDOUT\rdef pyDockerCmd(command):\rbash_exec = 'bash.exe'\rreturn Popen([bash_exec, '-c', f'{command}'], \\\rstdout = PIPE, stderr = PIPE, encoding='utf-8').communicate()\r in the command line interface (here I use Docker Quickstart terminal), you can test the result:\n$ python -c \u0026quot;import pyCmd; print(pyCmd.pyDockerCmd('docker run -it ubuntu bash'))\u0026quot;\r after execute, you will see something like this.\nIf you have something like this, it means at least you get rid of the \u0026ldquo;tty\u0026rdquo; error, which is annoying.\nThis \u0026ldquo;non-response\u0026rdquo; situation is actually because of the \u0026ldquo;print\u0026rdquo; command print all the response after the docker bash session end. You can try type \u0026ldquo;ls\u0026rdquo;, \u0026ldquo;pwd\u0026rdquo;,and \u0026ldquo;exit\u0026rdquo;. Since \u0026ldquo;exit\u0026rdquo; command will quit the bash terminal, you will see all the responses after you type \u0026ldquo;exit\u0026rdquo;. Now we know at least the python can interact with the \u0026ldquo;docker\u0026rdquo; freely without the \u0026ldquo;TTY\u0026rdquo; error. It is not perfect, we can see the responsivity is crappy. But at least we know it\u0026rsquo;s working.\n [bash terminal(Docker quickstart terminal)] ===\u0026gt; [golang.exec] ===\u0026gt; [python.subprocess] ===\u0026gt; [bash] ===\u0026gt; [docker command]\n   Now, if you wrap this python function into golang, and execute this in the Docker quickstart terminal, you will probably still see:\n ('', \u0026ldquo;the input device is not a TTY. If you are using mintty, try prefixing the command with \u0026lsquo;winpty\u0026rsquo;\\n\u0026rdquo;)\n wtf?\nThe golang function (testGoPy.go) is:\npackage main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;log\u0026quot;\r\u0026quot;os/exec\u0026quot;\r)\rfunc main() {\rout, err := exec.Command(\u0026quot;python\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;import pyCmd; print(pyCmd.pyDockerCmd('docker run -it ubuntu bash'))\u0026quot;).Output()\rif err != nil {\rlog.Fatal(err)\r}\rfmt.Printf(string(out))\r}\r Why is this?\nI stuck here for a while, until I found this post which is kind related to my problem. If we think this way, the command we want to execute by golang is docker run -it ubuntu bash, which is an interactive session, and it needs input and return output. However the golang exec.Command(...).Output() seems have no option of taking Stdin input. Hence, we need to modify the golang code so it can take stdin put.\nThe modified code is shown as the following.\npackage main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;os\u0026quot;\r\u0026quot;os/exec\u0026quot;\r)\rfunc main() {\rrunCommand(\u0026quot;python\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;import pyCmd; print(pyCmd.pyDockerCmd('docker run -it ubuntu bash'))\u0026quot;)\r}\rfunc runCommand(cmdName string, arg ...string) {\rcmd := exec.Command(cmdName, arg...)\rcmd.Stdout = os.Stdout\rcmd.Stderr = os.Stderr\rcmd.Stdin = os.Stdin\rerr := cmd.Run()\rif err != nil {\rfmt.Printf(err.Error())\ros.Exit(1)\r}\r}\r if you compile this and execute the this code in Docker quickstart terminal, it will work as before. (Similar to figure1 \u0026amp;2)\nAlthough it is not perfect, we have solved the problem of \u0026ldquo;is not a TTY\u0026rdquo;.\n  To summarize the issue  1. Try to use the \u0026quot;Docker quickstart terminal\u0026quot; instead of \u0026quot;git bash\u0026quot;. 2. in the golang ```exec``` command, instead of using ```exec.Command().Output()``` directly execute the command, using ```exec.run()``` and set up the **Stdin** and Stdout before execute the command.\r these two steps help me to solve my problem.\nFurther steps I know this piece of code is imperfect and there are many things to be improved. But at least this annoying \u0026ldquo;is not a tty\u0026rdquo; problem is solved. I will further investigate the python subprocess package and try to get more understandings about it.\n","date":"2017-11-26","permalink":"https://lmjw.github.io/post/2017-11-26-blog-docker-python-golang/","tags":["golang","python","docker","subprocess","bash"],"title":"An experiment using golang and python to execute command in docker"},{"content":"为了方便查看，我将本贴收录在我的博客中，请支持此贴的原作者，田琦的博客\n整理了下自己喜欢的软件，列出了这个清单。\n我的原则是：尽量使用免费开源软件，不使用破解软件。\n本文长期更新，欢迎推荐。\nWindows PicPick 瑞士军刀一样的软件。图像编辑器，颜色选择器，颜色调色板，像素标尺，量角器，瞄准线和白板等等，堪称全能的设计工具。更重要的是，它对个人用户是完全免费的。\n官网链接\nPuTTY Putty 是一个免费的，Windows 32 平台下的 telnet、rlogin 和 ssh 客户端。\n官网链接\nAdvanced ip Scanner 是可以在快速扫描局域网计算机信息的网络 IP 扫描工具，对于寻找一些没有显示的设备（比如没接屏幕的树莓派）IP 特别有用。\nOracle VM VirtualBox VMware Workstation 的绝佳开源替代品，十分适合在 Windows 中虚拟 Linux 系统环境。\n官网链接\nCmder cmd 替代品，能最大化，标签页以及非常不错的定制性。\n官网链接\ntypora Windows 下非常好用的 Markdown 编辑器，支持多种主题，更棒的是还支持 YAML 头文件。\n官网链接\nmp3Tag MP3 文件 ID3-Tag 信息修改器。可以修改 MP3 文件中的曲名、演唱者、专集、年月、流派、注释等信息，歌曲收藏者的利器。\n官网链接\nLinux zsh (with Oh My Zsh) 比 bash 更好用的 shell。更强的可配置性，更强的 tab 补全，还附带 git 支持。再加上Oh My Zsh的存在，zsh 已经相当易用了。\nOh My Zsh 项目链接\nTmux SSH 最佳伴侣。\n官网链接\nGraphviz “所想即所得”的画图工具，由大名鼎鼎的贝尔实验室开发。简单的来讲就是一款使用脚本语言来进行绘图的工具。\n官网链接\nDOT脚本语言\nGraph::Easy 跟 Graphviz 类似的软件。用它可以很方便的绘制出字符版的流程图，很适合代码注释。当然，它的功能远不止这些。\n{% highlight text%} +\u0026mdash;\u0026mdash;+ +\u0026mdash;\u0026mdash;\u0026ndash;+ \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;. +\u0026mdash;\u0026mdash;\u0026mdash;+ | Bonn | \u0026ndash;\u0026gt; | Berlin | \u0026ndash;\u0026gt; : Frankfurt : \u0026ndash;\u0026gt; | Dresden | +\u0026mdash;\u0026mdash;+ +\u0026mdash;\u0026mdash;\u0026ndash;+ \u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;. +\u0026mdash;\u0026mdash;\u0026mdash;+ : : v +\u0026mdash;\u0026mdash;\u0026mdash;+ +\u0026mdash;\u0026mdash;\u0026mdash;+ | Potsdam | ==\u0026gt; | Cottbus | +\u0026mdash;\u0026mdash;\u0026mdash;+ +\u0026mdash;\u0026mdash;\u0026mdash;+ {% endhighlight %}\nGitHub 项目页\nCoding Visual Studio Code 微软出品，和 atom 一样基于 electron，但比 atom 流畅。页面非常酷，插件也已经非常全面了，越来越多的前端工程师开始转向它了。\n官网链接\n","date":"2016-03-05","permalink":"https://lmjw.github.io/post/2016-03-05-my-favourit-softwares/","tags":["Tools"],"title":"我的软件清单"},{"content":"This is a forked syntax cheatsheet for Jekyll.\nView the markdown used to create this post.\n[View the markdown used to create this post](https://raw.githubusercontent.com/barryclark/www.jekyllnow.com/gh-pages/_posts/2014-6-19-Markdown-Style-Guide.md).\r  This is a paragraph, it\u0026rsquo;s surrounded by whitespace. Next up are some headers, they\u0026rsquo;re heavily influenced by GitHub\u0026rsquo;s markdown style.\nHeader 2 (H1 is reserved for post titles)## ## Header 2 (H1 is reserved for post titles)##\r Header 3 ### Header 3\r Header 4 #### Header 4\r  A link to Jekyll Now. A big ass literal link http://github.com/barryclark/jekyll-now/\nA link to [Jekyll Now](http://github.com/barryclark/jekyll-now/). A big ass literal link \u0026lt;http://github.com/barryclark/jekyll-now/\u0026gt;\r An image, located within /images\n![an image alt text]({{ site.baseurl }}/images/jekyll-logo.png \u0026ldquo;an image title\u0026rdquo;)\n![an image alt text]({{ site.baseurl }}/images/jekyll-logo.png \u0026quot;an image title\u0026quot;)\r   A bulletted list   alternative syntax 1   alternative syntax 2  an indented list item     An ordered list  * A bulletted list\r- alternative syntax 1\r+ alternative syntax 2\r- an indented list item\r1. An\r2. ordered\r3. list\r  Inline markup styles:\n italics  - _italics_\r  bold  - **bold**\r  code()  - `code()`\r  Blockquote\n \u0026gt; Blockquote\r   Nested Blockquote\n  \u0026gt;\u0026gt; Nested Blockquote\r  Syntax highlighting can be used with triple backticks, like so:\n/* Some pointless Javascript */\rvar rawr = [\u0026quot;r\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;w\u0026quot;, \u0026quot;r\u0026quot;];\r Use two trailing spaces\non the right\nto create linebreak tags\nFinally, horizontal lines\n ----\r  ****\r ","date":"2014-03-03","permalink":"https://lmjw.github.io/post/2014-3-3-markdown-style-guide/","tags":["Markdown CheatSheet"],"title":"Markdown Style Guide"}]